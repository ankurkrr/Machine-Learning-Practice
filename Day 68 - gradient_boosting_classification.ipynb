{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "P54W2Y4b4udC",
   "metadata": {
    "id": "P54W2Y4b4udC"
   },
   "source": [
    "# Gradient Boosting Algorithm - Part 2. Classification\n",
    "### Algorithm explained with an example, math, and code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2449e995",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-01-30T03:11:56.415032Z",
     "iopub.status.busy": "2022-01-30T03:11:56.414159Z",
     "iopub.status.idle": "2022-01-30T03:11:59.039366Z",
     "shell.execute_reply": "2022-01-30T03:11:59.038566Z",
     "shell.execute_reply.started": "2022-01-30T03:08:53.602090Z"
    },
    "id": "2449e995",
    "papermill": {
     "duration": 2.652237,
     "end_time": "2022-01-30T03:11:59.039550",
     "exception": false,
     "start_time": "2022-01-30T03:11:56.387313",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1013cf1f",
   "metadata": {
    "id": "1013cf1f",
    "papermill": {
     "duration": 0.014139,
     "end_time": "2022-01-30T03:11:59.071294",
     "exception": false,
     "start_time": "2022-01-30T03:11:59.057155",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Algorithm with an Example\n",
    "Gradient boosting is one of the variants of ensemble methods where you create multiple weak models (they are often decision trees) and combine them to get better performance as a whole. In this section, we are building a gradient boosting classification model using very simple example data to intuitively understand how it works.\n",
    "\n",
    "The picture below shows the example data. It has the binary class $y$ (0 and 1) and two features ($x₁$ and $x₂$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88023d9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "execution": {
     "iopub.execute_input": "2022-01-30T03:11:59.109145Z",
     "iopub.status.busy": "2022-01-30T03:11:59.108103Z",
     "iopub.status.idle": "2022-01-30T03:11:59.445414Z",
     "shell.execute_reply": "2022-01-30T03:11:59.445968Z",
     "shell.execute_reply.started": "2022-01-30T03:08:53.610078Z"
    },
    "id": "d88023d9",
    "outputId": "af628356-028d-4d1e-d91b-10af08b14dc8",
    "papermill": {
     "duration": 0.360591,
     "end_time": "2022-01-30T03:11:59.446139",
     "exception": false,
     "start_time": "2022-01-30T03:11:59.085548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sklearn.datasets as datasets\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "data = datasets.make_circles(n_samples=100, factor=0.5, noise=0.15, random_state=0)\n",
    "x, y = data[0], data[1]\n",
    "\n",
    "# make it imbalance\n",
    "idx = np.sort(np.append(np.where(y != 0)[0], np.where(y == 0)[0][:-10]))\n",
    "x, y = x[idx], y[idx]\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.scatter(x[y==0, 0], x[y==0, 1], c='orange', edgecolors='w', s=100, label='class 0')\n",
    "plt.scatter(x[y==1, 0], x[y==1, 1], c='crimson', edgecolors='w', s=100, label='class 1')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.legend(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f03a3cf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "execution": {
     "iopub.execute_input": "2022-01-30T03:11:59.487207Z",
     "iopub.status.busy": "2022-01-30T03:11:59.486065Z",
     "iopub.status.idle": "2022-01-30T03:11:59.674460Z",
     "shell.execute_reply": "2022-01-30T03:11:59.673932Z",
     "shell.execute_reply.started": "2022-01-30T03:08:53.825193Z"
    },
    "id": "8f03a3cf",
    "outputId": "615ccd33-5347-40be-a008-2dde0a0be5fa",
    "papermill": {
     "duration": 0.213001,
     "end_time": "2022-01-30T03:11:59.674624",
     "exception": false,
     "start_time": "2022-01-30T03:11:59.461623",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "c_scale = [[0, \"rgb(247, 168, 84)\"], [1, \"rgb(209, 0, 0)\"]]\n",
    "\n",
    "def create_scatter(x, y, plot_type):\n",
    "\n",
    "    return go.Scatter3d(\n",
    "                x=x[:, 0], y=x[:, 1], z=y,\n",
    "                mode=\"markers\",\n",
    "                marker=dict(\n",
    "                    size=6,\n",
    "                    color=y,\n",
    "                    colorscale=c_scale if plot_type == \"pred\" else \"Blugrn\",\n",
    "                    line=dict(width=4, color=\"White\"),\n",
    "                ),\n",
    "            )\n",
    "\n",
    "def format_plot(fig):\n",
    "  return fig.update_layout(\n",
    "      scene=dict(\n",
    "          xaxis_title=\"x1\",\n",
    "          yaxis_title=\"x2\",\n",
    "          zaxis_title=\"y\"\n",
    "      ),\n",
    "      height=400,\n",
    "      width=600,\n",
    "      margin=dict(l=10, r=10, t=40, b=20),\n",
    "  )\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(create_scatter(x, y, \"pred\"))\n",
    "fig = format_plot(fig)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JMdvq_cH6NLf",
   "metadata": {
    "id": "JMdvq_cH6NLf"
   },
   "source": [
    "Our goal is to build a gradient boosting model that classifies those two classes. The first step is making a uniform prediction on a probability of class 1 (we will call it $p$) for all the data points. The most reasonable value for the uniform prediction might be the proportion of class 1 which is just a mean of $y$.\n",
    "\n",
    "$$p = P(y=1) = \\bar{y}$$\n",
    "\n",
    "Here is a 3D representation of the data and the initial prediction. At this moment, the prediction is just a plane which has the uniform value $p = mean(y)$ on the $y$ axis all the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89ee424",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T03:11:59.715171Z",
     "iopub.status.busy": "2022-01-30T03:11:59.714176Z",
     "iopub.status.idle": "2022-01-30T03:11:59.724020Z",
     "shell.execute_reply": "2022-01-30T03:11:59.724829Z",
     "shell.execute_reply.started": "2022-01-30T03:08:53.856170Z"
    },
    "id": "a89ee424",
    "papermill": {
     "duration": 0.033187,
     "end_time": "2022-01-30T03:11:59.725099",
     "exception": false,
     "start_time": "2022-01-30T03:11:59.691912",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# creating mesh data to visualize prediction planes\n",
    "x1_min, x1_max = x[:, 0].min() - 0.5, x[:, 0].max() + 0.5\n",
    "x2_min, x2_max = x[:, 1].min() - 0.5, x[:, 1].max() + 0.5\n",
    "\n",
    "h = 0.02  # step size in the mesh\n",
    "x1_mesh, x2_mesh = np.meshgrid(np.arange(x1_min, x1_max, h), np.arange(x2_min, x2_max, h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YLZof_xzHcBi",
   "metadata": {
    "id": "YLZof_xzHcBi"
   },
   "outputs": [],
   "source": [
    "def create_surface(x1, x2, y, plot_type):\n",
    "\n",
    "    return go.Surface(x=x1, y=x2, z=y,\n",
    "                      showscale=False,\n",
    "                      opacity=0.5,\n",
    "                      colorscale=\"Peach\" if plot_type == \"pred\" else \"Tealgrn\",\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a731c32c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "execution": {
     "iopub.execute_input": "2022-01-30T03:11:59.774918Z",
     "iopub.status.busy": "2022-01-30T03:11:59.773916Z",
     "iopub.status.idle": "2022-01-30T03:11:59.866793Z",
     "shell.execute_reply": "2022-01-30T03:11:59.867334Z",
     "shell.execute_reply.started": "2022-01-30T03:08:53.866702Z"
    },
    "id": "a731c32c",
    "outputId": "087c4244-dc51-47ee-8adf-9b60b47e20bc",
    "papermill": {
     "duration": 0.118927,
     "end_time": "2022-01-30T03:11:59.867519",
     "exception": false,
     "start_time": "2022-01-30T03:11:59.748592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "p = y.mean()\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(create_scatter(x, y, \"pred\"))\n",
    "fig.add_trace(create_surface(x1_mesh, x2_mesh, np.full(x1_mesh.shape, p), \"pred\"))\n",
    "fig = format_plot(fig)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rWo45PeK6yEi",
   "metadata": {
    "id": "rWo45PeK6yEi"
   },
   "source": [
    "In our data, the mean of $y$ is 0.56. As it is bigger than 0.5, everything is classified into class 1 with this initial prediction. Some of you might feel that this uniform value prediction does not make sense, but don't worry. We will improve our prediction as we add more weak models to it.\n",
    "\n",
    "To improve our prediction quality, we might want to focus on the residuals (i.e. prediction error) from our initial prediction as that is what we want to minimize. The residuals are defined as $rᵢ = yᵢ − p$ ($i$ represents the index of each data point). In the figure below, the residuals are shown as the brown lines that are the perpendicular lines from each data point to the prediction plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9PDcsycmHpaZ",
   "metadata": {
    "id": "9PDcsycmHpaZ"
   },
   "outputs": [],
   "source": [
    "def create_residual_lines(x, y, pred):\n",
    "\n",
    "    #create the coordinate list for the lines\n",
    "    x_list, y_list, z_list = [], [], []\n",
    "    for i in range(len(x)):\n",
    "        x_list.extend([x[i, 0], x[i, 0], None])\n",
    "        y_list.extend([x[i, 1], x[i, 1], None])\n",
    "        z_list.extend([y[i], pred[i], None])\n",
    "\n",
    "    return go.Scatter3d(x=x_list,\n",
    "                         y=y_list,\n",
    "                         z=z_list,\n",
    "                         mode='lines',\n",
    "                         line=dict(\n",
    "                             color='brown',\n",
    "                             width=5\n",
    "                         ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f046fcb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "execution": {
     "iopub.execute_input": "2022-01-30T03:11:59.994490Z",
     "iopub.status.busy": "2022-01-30T03:11:59.993471Z",
     "iopub.status.idle": "2022-01-30T03:12:00.108364Z",
     "shell.execute_reply": "2022-01-30T03:12:00.108918Z",
     "shell.execute_reply.started": "2022-01-30T03:08:53.918855Z"
    },
    "id": "4f046fcb",
    "outputId": "defe4085-5346-429b-abe8-f634106b65fb",
    "papermill": {
     "duration": 0.183728,
     "end_time": "2022-01-30T03:12:00.109091",
     "exception": false,
     "start_time": "2022-01-30T03:11:59.925363",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(create_scatter(x, y, \"pred\"))\n",
    "fig.add_trace(create_surface(x1_mesh, x2_mesh, np.full(x1_mesh.shape, p), \"pred\"))\n",
    "fig.add_trace(create_residual_lines(x, y, np.full(y.shape, p)))\n",
    "fig = format_plot(fig)\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KClCpJds7DIA",
   "metadata": {
    "id": "KClCpJds7DIA"
   },
   "source": [
    "To minimize these residuals, we are building a regression tree model with both $x₁$ and $x₂$ as its features and the residuals r as its target. If we can build a tree that finds some patterns between $x$ and $r$, we can reduce the residuals by utilizing that created tree.\n",
    "To simplify the demonstration, we are building very simple trees each of that only has one split and two terminal nodes which is called \"stump\". Please note that gradient boosting trees usually have a little deeper trees such as ones with 8 to 32 terminal nodes.\n",
    "Here we are creating the first tree predicting the residuals with two different values $r = \\{0.1, -0.6\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31177ac7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T03:12:00.310909Z",
     "iopub.status.busy": "2022-01-30T03:12:00.309806Z",
     "iopub.status.idle": "2022-01-30T03:12:00.311882Z",
     "shell.execute_reply": "2022-01-30T03:12:00.312467Z",
     "shell.execute_reply.started": "2022-01-30T03:08:53.982680Z"
    },
    "id": "31177ac7",
    "papermill": {
     "duration": 0.106902,
     "end_time": "2022-01-30T03:12:00.312663",
     "exception": false,
     "start_time": "2022-01-30T03:12:00.205761",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_and_update(x, y, Fm, x1_mesh, x2_mesh, Fm_mesh, learing_rate=0.1, print_tree=True):\n",
    "\n",
    "    p = np.exp(Fm) / (1 + np.exp(Fm))\n",
    "    r = y - p\n",
    "    tree = DecisionTreeRegressor(max_depth=1, random_state=0)\n",
    "    tree.fit(x, r)\n",
    "    ids = tree.apply(x)\n",
    "\n",
    "    if print_tree:\n",
    "      print_tree_structure(tree)\n",
    "\n",
    "    x_mesh = np.c_[x1_mesh.ravel(), x2_mesh.ravel()]\n",
    "    r_pred_mesh = tree.predict(x_mesh).reshape(x1_mesh.shape)\n",
    "\n",
    "    for j in np.unique(ids):\n",
    "        fltr = ids == j\n",
    "        num = r[fltr].sum()\n",
    "        den = (p[fltr]*(1-p[fltr])).sum()\n",
    "        gamma = num / den\n",
    "        Fm[fltr] += learing_rate * gamma\n",
    "\n",
    "        # update prediction value in the tree\n",
    "        tree.tree_.value[j, 0, 0] = gamma\n",
    "\n",
    "    gamma_update_mesh = tree.predict(x_mesh).reshape(x1_mesh.shape)\n",
    "    Fm_mesh += learing_rate * gamma_update_mesh\n",
    "\n",
    "    p_mesh = np.exp(Fm_mesh) / (1 + np.exp(Fm_mesh))\n",
    "\n",
    "    return tree, r, Fm, r_pred_mesh, Fm_mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UhgynwlAANtz",
   "metadata": {
    "id": "UhgynwlAANtz"
   },
   "outputs": [],
   "source": [
    "# this function print out tree structures. adapted from https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html\n",
    "def print_tree_structure(clf):\n",
    "\n",
    "    n_nodes = clf.tree_.node_count\n",
    "    children_left = clf.tree_.children_left\n",
    "    children_right = clf.tree_.children_right\n",
    "    feature = clf.tree_.feature\n",
    "    threshold = clf.tree_.threshold\n",
    "\n",
    "    node_depth = np.zeros(shape=n_nodes, dtype=np.int64)\n",
    "    is_leaves = np.zeros(shape=n_nodes, dtype=bool)\n",
    "    stack = [(0, 0)]  # start with the root node id (0) and its depth (0)\n",
    "    while len(stack) > 0:\n",
    "        # `pop` ensures each node is only visited once\n",
    "        node_id, depth = stack.pop()\n",
    "        node_depth[node_id] = depth\n",
    "\n",
    "        # If the left and right child of a node is not the same we have a split\n",
    "        # node\n",
    "        is_split_node = children_left[node_id] != children_right[node_id]\n",
    "        # If a split node, append left and right children and depth to `stack`\n",
    "        # so we can loop through them\n",
    "        if is_split_node:\n",
    "            stack.append((children_left[node_id], depth + 1))\n",
    "            stack.append((children_right[node_id], depth + 1))\n",
    "        else:\n",
    "            is_leaves[node_id] = True\n",
    "\n",
    "    print('-'*80)\n",
    "    print(\n",
    "        \"The binary tree structure has {n} nodes and has \"\n",
    "        \"the following tree structure:\\n\".format(n=n_nodes)\n",
    "    )\n",
    "    for i in range(n_nodes):\n",
    "        if is_leaves[i]:\n",
    "            print(\n",
    "                \"{space}node={node} is a leaf node.\".format(\n",
    "                    space=node_depth[i] * \"\\t\", node=i\n",
    "                )\n",
    "            )\n",
    "            print(node_depth[i] * '\\t', f\"prediction: {clf.tree_.value[i, 0, 0]:.1f}\")\n",
    "        else:\n",
    "            print(\n",
    "                \"{space}node={node} is a split node: \"\n",
    "                \"go to node {left} if X[:, {feature}] <= {threshold} \"\n",
    "                \"else to node {right}.\".format(\n",
    "                    space=node_depth[i] * \"\\t\",\n",
    "                    node=i,\n",
    "                    left=children_left[i],\n",
    "                    feature=feature[i],\n",
    "                    threshold=threshold[i],\n",
    "                    right=children_right[i],\n",
    "                )\n",
    "            )\n",
    "    print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62e6019",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "execution": {
     "iopub.execute_input": "2022-01-30T03:12:00.503248Z",
     "iopub.status.busy": "2022-01-30T03:12:00.502243Z",
     "iopub.status.idle": "2022-01-30T03:12:00.577618Z",
     "shell.execute_reply": "2022-01-30T03:12:00.578141Z",
     "shell.execute_reply.started": "2022-01-30T03:08:53.993752Z"
    },
    "id": "e62e6019",
    "outputId": "2ebcce88-7574-43c8-9b46-33c1cf4032fc",
    "papermill": {
     "duration": 0.171912,
     "end_time": "2022-01-30T03:12:00.578310",
     "exception": false,
     "start_time": "2022-01-30T03:12:00.406398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "F0 = np.log(p/(1-p))\n",
    "F0 = np.full(len(y), F0)\n",
    "F0_mesh = np.full(x1_mesh.shape, F0[0])\n",
    "learing_rate = 0.9\n",
    "tree, r, Fm, r_pred_mesh, Fm_mesh = train_and_update(x, y, F0, x1_mesh, x2_mesh, F0_mesh, learing_rate=learing_rate)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(create_scatter(x, r, \"res\"))\n",
    "fig.add_trace(create_surface(x1_mesh, x2_mesh, r_pred_mesh, \"res\"))\n",
    "fig = format_plot(fig)\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3xR6PqIM7qnN",
   "metadata": {
    "id": "3xR6PqIM7qnN"
   },
   "source": [
    "You might now think we want to add these predicted values to our initial prediction $p$ to reduce its residuals if you already read [the post talking about the regression algorithm](https://medium.com/p/2520a34a502), but things are slightly different with the classification. The values (we call it $γ$ gamma) that we are adding to our initial prediction is computed in the following formula:\n",
    "\n",
    "$$\\frac{\\sum_{x_{i}\\in{R_{j}}}(y_{i}-p)}{\\sum_{x_{i}\\in{R_{j}}}p(1-p)} $$\n",
    "\n",
    "$Σxᵢ∈Rⱼ$ means we are aggregating the values in the sigma $Σ$ on all the sample $xᵢ$s that belong to the terminal node $Rⱼ$. $j$ represents the index of each terminal node. You might notice that the numerator of the fraction is the sum of the residuals in the terminal node j. We will go through all the calculations that give us this formula, but let's just use it to calculate $γ$ for now. Below is the computed values of $γ₁$ and $γ₂$.\n",
    "\n",
    "$$γ_{1}=\\frac{\\sum_{x_{i}\\in{R_{1}}}(y_{i}-0.56)}{\\sum_{x_{i}\\in{R_{1}}}0.56(1-0.56)}=0.3$$\n",
    "\n",
    "$$γ_{2}=\\frac{\\sum_{x_{i}\\in{R_{2}}}(y_{i}-0.56)}{\\sum_{x_{i}\\in{R_{2}}}0.56(1-0.56)}=-2.2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DnvEG6boJenx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DnvEG6boJenx",
    "outputId": "e76b2d2a-e575-4ae7-9586-8d85e0f6a911"
   },
   "outputs": [],
   "source": [
    "print_tree_structure(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w1Q9UmtqJkXP",
   "metadata": {
    "id": "w1Q9UmtqJkXP"
   },
   "source": [
    "This $γ$ is not simply added to our initial prediction p. Instead, we are converting p into log-odds (we will call this log-odds converted value $F(x)$), then adding γ to it. For those who are not familiar with log-odds, it is defined below. You might have seen it used in [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression).\n",
    "\n",
    "$$ log(odds)=log(\\frac{p}{1-p}) $$\n",
    "\n",
    "One more tweak on the prediction update is that γ is scaled down by **learning rate** $ν$, which ranges between 0 and 1, and then added to the log-odds-converted prediction$F(x)$. This helps the model not to overfit the training data.\n",
    "\n",
    "$$ F_{1}(x)=F_{0}(x)+ ν∙γ$$\n",
    "\n",
    "In this example, we use a relatively big learning rate $ν = 0.9$ to make the optimization process easier to understand, but it is usually supposed to be a much smaller value such as 0.1.\n",
    "By substituting actual values for the variables in the right side of the above equation, we get our updated prediction F₁(x).\n",
    "\n",
    "$$\n",
    "F₁(x) = \\begin{cases}\n",
    "    log(\\frac{0.56}{1-0.56})+0.9∙0.3=0.5 & \\text{if } x≤0.64 \\\\\n",
    "    log(\\frac{0.56}{1-0.56})-0.9∙2.2=-1.7 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "If we convert log-odds $F(x)$ back into the predicted probability $p(x)$ (we will cover how we can convert it in the next section), it looks like a stair-like object below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3GZg8DlPOgQ7",
   "metadata": {
    "id": "3GZg8DlPOgQ7"
   },
   "outputs": [],
   "source": [
    "def create_prev_surface(x1, x2, y, plot_type):\n",
    "\n",
    "    return go.Surface(x=x1, y=x2, z=y,\n",
    "                      showscale=False,\n",
    "                      opacity=0.5,\n",
    "                      colorscale=\"Purples\",\n",
    "                      surfacecolor=np.ones(x1.shape),\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf61bdf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "execution": {
     "iopub.execute_input": "2022-01-30T03:12:00.848475Z",
     "iopub.status.busy": "2022-01-30T03:12:00.847410Z",
     "iopub.status.idle": "2022-01-30T03:12:00.915416Z",
     "shell.execute_reply": "2022-01-30T03:12:00.915975Z",
     "shell.execute_reply.started": "2022-01-30T03:08:54.054108Z"
    },
    "id": "bbf61bdf",
    "outputId": "981f8bca-7cb2-49ee-bb1e-663770a78598",
    "papermill": {
     "duration": 0.204822,
     "end_time": "2022-01-30T03:12:00.916144",
     "exception": false,
     "start_time": "2022-01-30T03:12:00.711322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "p_mesh = np.exp(Fm_mesh) / (1 + np.exp(Fm_mesh))\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(create_scatter(x, y, \"pred\"))\n",
    "fig.add_trace(create_prev_surface(x1_mesh, x2_mesh, np.full(x1_mesh.shape, p), \"pred\"))\n",
    "fig.add_trace(create_surface(x1_mesh, x2_mesh, p_mesh, \"pred\"))\n",
    "fig = format_plot(fig)\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kuDjIjk3_rX_",
   "metadata": {
    "id": "kuDjIjk3_rX_"
   },
   "source": [
    "The purple-colored plane is the initial prediction $p₀$ and it is updated to the red and yellow plane $p₁$.\n",
    "Now, the updated residuals $r$ looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110550a8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "execution": {
     "iopub.execute_input": "2022-01-30T03:12:01.267688Z",
     "iopub.status.busy": "2022-01-30T03:12:01.266783Z",
     "iopub.status.idle": "2022-01-30T03:12:01.348376Z",
     "shell.execute_reply": "2022-01-30T03:12:01.348924Z",
     "shell.execute_reply.started": "2022-01-30T03:08:54.102937Z"
    },
    "id": "110550a8",
    "outputId": "ebdfe0ef-2c30-4292-985d-fbd6d8c460ff",
    "papermill": {
     "duration": 0.259074,
     "end_time": "2022-01-30T03:12:01.349098",
     "exception": false,
     "start_time": "2022-01-30T03:12:01.090024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "p = np.exp(Fm) / (1 + np.exp(Fm))\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(create_scatter(x, y, \"pred\"))\n",
    "fig.add_trace(create_surface(x1_mesh, x2_mesh, p_mesh, \"pred\"))\n",
    "fig.add_trace(create_residual_lines(x, y, p))\n",
    "fig = format_plot(fig)\n",
    "fig.update_layout(title=\"Updated Residuals\", showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AIYwLJl3_y8v",
   "metadata": {
    "id": "AIYwLJl3_y8v"
   },
   "source": [
    "In the next step, we are creating a regression tree again using the same $x_{1}$ and $x_{2}$ as the features and the updated residuals $r$ as its target. Here is the created tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f5e3ea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "execution": {
     "iopub.execute_input": "2022-01-30T03:12:01.788625Z",
     "iopub.status.busy": "2022-01-30T03:12:01.783312Z",
     "iopub.status.idle": "2022-01-30T03:12:01.857627Z",
     "shell.execute_reply": "2022-01-30T03:12:01.858163Z",
     "shell.execute_reply.started": "2022-01-30T03:08:54.160026Z"
    },
    "id": "49f5e3ea",
    "outputId": "9e9e6cd9-0ac6-4e26-a79e-eb55b2b5aa4a",
    "papermill": {
     "duration": 0.293131,
     "end_time": "2022-01-30T03:12:01.858335",
     "exception": false,
     "start_time": "2022-01-30T03:12:01.565204",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "p_mesh_prev = p_mesh\n",
    "tree, r, Fm, r_pred_mesh, Fm_mesh = train_and_update(x, y, Fm, x1_mesh, x2_mesh, Fm_mesh, learing_rate=learing_rate)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(create_scatter(x, r, \"res\"))\n",
    "fig.add_trace(create_surface(x1_mesh, x2_mesh, r_pred_mesh, \"res\"))\n",
    "fig = format_plot(fig)\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Vr2B2DomAIAy",
   "metadata": {
    "id": "Vr2B2DomAIAy"
   },
   "source": [
    "We apply the same formula to compute $γ$. The calculated $γ$ along with the updated prediction F₂(x) are as follows.\n",
    "\n",
    "$$\n",
    "F_{2}(x) = \\begin{cases}\n",
    "    F_{1}(x)-ν∙2.3=0.5-0.9⋅2.3=-1.6 & \\text{if } x_{1}≤-0.63 \\\\\n",
    "    F_{1}(x)+ν∙0.4=0.5+0.9⋅0.4=0.9 & \\text{else if } -0.63<x_{1}≤0.64 \\\\\n",
    "    F_{1}(x)+ν∙0.4=-1.7+0.9⋅0.4=-1.3 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sHl3mMacJG1r",
   "metadata": {
    "id": "sHl3mMacJG1r"
   },
   "source": [
    "Again, if we convert log-odds $F₂(x)$ back into the predicted probability $p₂(x)$, it looks like something below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ea07c6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "execution": {
     "iopub.execute_input": "2022-01-30T03:12:02.377673Z",
     "iopub.status.busy": "2022-01-30T03:12:02.376984Z",
     "iopub.status.idle": "2022-01-30T03:12:02.453470Z",
     "shell.execute_reply": "2022-01-30T03:12:02.454032Z",
     "shell.execute_reply.started": "2022-01-30T03:08:54.219259Z"
    },
    "id": "d4ea07c6",
    "outputId": "8253d223-6ff1-46e4-d623-f1484c650894",
    "papermill": {
     "duration": 0.337782,
     "end_time": "2022-01-30T03:12:02.454215",
     "exception": false,
     "start_time": "2022-01-30T03:12:02.116433",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "p_mesh = np.exp(Fm_mesh) / (1 + np.exp(Fm_mesh))\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(create_scatter(x, y, \"pred\"))\n",
    "\n",
    "fig.add_trace(create_prev_surface(x1_mesh, x2_mesh, p_mesh_prev, \"pred\"))\n",
    "fig.add_trace(create_surface(x1_mesh, x2_mesh, p_mesh, \"pred\"))\n",
    "p = np.exp(Fm) / (1 + np.exp(Fm))\n",
    "# fig.add_trace(create_residual_lines(x, y, pred))\n",
    "fig = format_plot(fig)\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OpuZvxUnAjB7",
   "metadata": {
    "id": "OpuZvxUnAjB7"
   },
   "source": [
    "We iterate these steps until the model prediction stops improving. The figures below show the optimization process from 0 to 4 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0075ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-30T03:12:03.058771Z",
     "iopub.status.busy": "2022-01-30T03:12:03.057667Z",
     "iopub.status.idle": "2022-01-30T03:12:03.059664Z",
     "shell.execute_reply": "2022-01-30T03:12:03.060243Z",
     "shell.execute_reply.started": "2022-01-30T03:08:54.286639Z"
    },
    "id": "2f0075ca",
    "papermill": {
     "duration": 0.311829,
     "end_time": "2022-01-30T03:12:03.060408",
     "exception": false,
     "start_time": "2022-01-30T03:12:02.748579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_subplots(iter_num):\n",
    "\n",
    "    return make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        specs=[[{\"is_3d\": True}, {\"is_3d\": True}]],\n",
    "        horizontal_spacing=0,\n",
    "        subplot_titles=(f\"Residuals of iteration {iter_num}\", f\"Predictions of iteration {iter_num}\"),\n",
    "    )\n",
    "\n",
    "def format_subplots(fig):\n",
    "    fig.update_layout(\n",
    "        scene=dict(\n",
    "            xaxis_title=\"x1\",\n",
    "            yaxis_title=\"x2\",\n",
    "            zaxis_title=\"y\"\n",
    "        ),\n",
    "        scene2=dict(\n",
    "            xaxis_title=\"x1\",\n",
    "            yaxis_title=\"x2\",\n",
    "            zaxis_title=\"y\"\n",
    "        ),\n",
    "        height=400,\n",
    "        width=1200,\n",
    "        margin=dict(l=10, r=10, t=20, b=20),\n",
    "    )\n",
    "    fig.update_layout(showlegend=False)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd07a7d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2022-01-30T03:12:03.655349Z",
     "iopub.status.busy": "2022-01-30T03:12:03.645881Z",
     "iopub.status.idle": "2022-01-30T03:12:04.313576Z",
     "shell.execute_reply": "2022-01-30T03:12:04.314112Z",
     "shell.execute_reply.started": "2022-01-30T03:08:54.295594Z"
    },
    "id": "9fd07a7d",
    "outputId": "088f2613-a20e-4db6-9f93-61e13e71a65d",
    "papermill": {
     "duration": 0.963448,
     "end_time": "2022-01-30T03:12:04.314289",
     "exception": false,
     "start_time": "2022-01-30T03:12:03.350841",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "p = y.mean()\n",
    "F0 = np.log(p/(1-p))\n",
    "\n",
    "learing_rate = 0.9\n",
    "Fm_mesh = np.full(x1_mesh.shape, F0)\n",
    "Fm = np.full(len(y), F0)\n",
    "n_estimators = 4\n",
    "p = y.mean()\n",
    "\n",
    "fig = create_subplots(0)\n",
    "fig.append_trace(create_scatter(x, y, \"pred\"), row=1, col=2)\n",
    "fig.append_trace(create_surface(x1_mesh, x2_mesh, np.full(x1_mesh.shape, p), \"pred\"), row=1, col=2)\n",
    "fig = format_subplots(fig)\n",
    "fig.show()\n",
    "\n",
    "for i in range(n_estimators):\n",
    "    tree, r, Fm, r_pred_mesh, Fm_mesh = train_and_update(x, y, Fm, x1_mesh, x2_mesh, Fm_mesh,\n",
    "                                                         learing_rate=learing_rate, print_tree=False)\n",
    "    p_mesh = np.exp(Fm_mesh) / (1 + np.exp(Fm_mesh))\n",
    "\n",
    "    fig = create_subplots(i+1)\n",
    "    fig.append_trace(create_scatter(x, r, \"res\"), row=1, col=1)\n",
    "    fig.append_trace(create_surface(x1_mesh, x2_mesh, r_pred_mesh, \"res\"), row=1, col=1)\n",
    "    fig.append_trace(create_scatter(x, y, \"pred\"), row=1, col=2)\n",
    "    fig.append_trace(create_surface(x1_mesh, x2_mesh, p_mesh, \"pred\"), row=1, col=2)\n",
    "\n",
    "    fig = format_subplots(fig)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TFQLXk2vCybH",
   "metadata": {
    "id": "TFQLXk2vCybH"
   },
   "source": [
    "You can see the combined prediction $p(x)$ (red and yellow plane) is getting closer to our target $y$ as we add more trees into the combined model. This is how gradient boosting works to predict complex targets by combining multiple weak models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_ruLAlfZArh2",
   "metadata": {
    "id": "_ruLAlfZArh2"
   },
   "source": [
    "# Code\n",
    "In this section, we are translating the maths we just reviewed into a viable python code to help us understand the algorithm further. We are using `DecisionTreeRegressor` from scikit-learn to build trees which helps us just focus on the gradient boosting algorithm itself instead of the tree algorithm. We are imitating scikit-learn style implementation where you train the model with `fit` method and make predictions with `predict` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "W_uhdo4p4lSx",
   "metadata": {
    "id": "W_uhdo4p4lSx"
   },
   "outputs": [],
   "source": [
    "class CustomGradientBoostingClassifier:\n",
    "\n",
    "    def __init__(self, learning_rate, n_estimators, max_depth=1):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        F0 = np.log(y.mean()/(1-y.mean()))  # log-odds values\n",
    "        self.F0 = np.full(len(y), F0)  # converting to array with the input length\n",
    "        Fm = self.F0.copy()\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            p = np.exp(Fm) / (1 + np.exp(Fm))  # converting back to probabilities\n",
    "            r = y - p  # residuals\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth, random_state=0)\n",
    "            tree.fit(X, r)\n",
    "            ids = tree.apply(x)  # getting the terminal node IDs\n",
    "\n",
    "            # looping through the terminal nodes\n",
    "            for j in np.unique(ids):\n",
    "              fltr = ids == j\n",
    "\n",
    "              # getting gamma using the formula (Σresiduals/Σp(1-p))\n",
    "              num = r[fltr].sum()\n",
    "              den = (p[fltr]*(1-p[fltr])).sum()\n",
    "              gamma = num / den\n",
    "\n",
    "              # updating the prediction\n",
    "              Fm[fltr] += self.learning_rate * gamma\n",
    "\n",
    "              # replacing the prediction value in the tree\n",
    "              tree.tree_.value[j, 0, 0] = gamma\n",
    "\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "\n",
    "        Fm = self.F0\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            Fm += self.learning_rate * self.trees[i].predict(X)\n",
    "\n",
    "        return np.exp(Fm) / (1 + np.exp(Fm))  # converting back to probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HdmHe5FaAvKZ",
   "metadata": {
    "id": "HdmHe5FaAvKZ"
   },
   "source": [
    "Please note that all the trained trees are stored in `self.trees` list object and it is retrieved when we make predictions with `predict_proba` method.\n",
    "Next, we are checking if our `CustomGradientBoostingClassifier` performs as the same as `GradientBoostingClassifier` from scikit-learn by looking at their log-loss on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YY8Z5MoL-iaH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YY8Z5MoL-iaH",
    "outputId": "914b5e86-9c2f-4236-dac0-7aa16d06443d"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "custom_gbm = CustomGradientBoostingClassifier(\n",
    "    n_estimators=20,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=1\n",
    ")\n",
    "custom_gbm.fit(x, y)\n",
    "custom_gbm_log_loss = log_loss(y, custom_gbm.predict_proba(x))\n",
    "print(f\"Custom GBM Log-Loss:{custom_gbm_log_loss:.15f}\")\n",
    "\n",
    "sklearn_gbm = GradientBoostingClassifier(\n",
    "    n_estimators=20,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=1\n",
    ")\n",
    "sklearn_gbm.fit(x, y)\n",
    "sklearn_gbm_log_loss = log_loss(y, sklearn_gbm.predict_proba(x))\n",
    "print(f\"Scikit-learn GBM Log-Loss:{sklearn_gbm_log_loss:.15f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aQ-_5fnA3lp",
   "metadata": {
    "id": "8aQ-_5fnA3lp"
   },
   "source": [
    "As you can see in the output above, both models have exactly the same log-loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Sn-XQi2ZMArR",
   "metadata": {
    "id": "Sn-XQi2ZMArR"
   },
   "source": [
    "# Recommended Resources\n",
    "In this notebook, we have reviewed all the details of the gradient boosting classification algorithm. If you are also interested in the regression algorithm, please look at the [Part 1 notebook](https://github.com/tomonori-masui/gradient-boosting/blob/main/gradient_boosting_regression.ipynb).\n",
    "\n",
    "There are also some other great resources if you want further details of the algorithm:\n",
    "- **StatQuest, Gradient Boost [Part3](https://www.youtube.com/watch?v=jxuNLH5dXCs) and [Part 4](https://www.youtube.com/watch?v=StWY5QWMXCw)**\n",
    "\n",
    "These are the YouTube videos explaining the gradient boosting classification algorithm with great visuals in a beginner-friendly way.\n",
    "\n",
    "- **Terence Parr and Jeremy Howard, [How to explain gradient boosting](https://explained.ai/gradient-boosting/index.html)**\n",
    "\n",
    "While this article focuses on gradient boosting regression instead of classification, it nicely explains every detail of the algorithm.\n",
    "\n",
    "- **Jerome Friedman, [Greedy Function Approximation: A Gradient Boosting Machine](https://jerryfriedman.su.domains/ftp/trebst.pdf)**\n",
    "\n",
    "This is the original paper from Friedman.\n",
    "While it is a little hard to understand, it surely shows the flexibility of the algorithm where he shows a generalized algorithm that can deal with any type of problem having a differentiable loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iMbYZAQpM7wL",
   "metadata": {
    "id": "iMbYZAQpM7wL"
   },
   "source": [
    "# References\n",
    "- Jerome Friedman, Greedy Function Approximation: A Gradient Boosting Machine\n",
    "- Terence Parr and Jeremy Howard, How to explain gradient boosting\n",
    "- Matt Bowers, How to Build a Gradient Boosting Machine from Scratch\n",
    "- Wikipedia, Gradient boosting"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 21.321233,
   "end_time": "2022-01-30T03:12:07.129100",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-01-30T03:11:45.807867",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
